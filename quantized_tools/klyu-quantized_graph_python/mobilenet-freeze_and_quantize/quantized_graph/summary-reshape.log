
(tf-py2-quan) ydwu@aries:~/framework/tensorflow22/tensorflow$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/home/ydwu/framework/tensorflow22/tensorflow/ydwu-learning_tf/quantized_platerec_model_graph_10928f2_opt_reshape.pb --print_structure=true

Found 1 possible inputs: (name=Placeholder, type=float(1), shape=None) 
No variables spotted.
Found 1 possible outputs: (name=ArgMax, op=ArgMax) 
Found 3819716 (3.82M) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 229 Const, 28 QuantizedBiasAdd, 27 QuantizedRelu, 14 QuantizedConv2D, 13 QuantizedDepthwiseConv, 2 QuantizedReshape, 1 ArgMax, 1 Dequantize, 1 ExpandDims, 1 Max, 1 Min, 1 Placeholder, 1 QuantizeV2, 1 QuantizedAvgPool, 1 QuantizedMatMul, 1 Reshape
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/ydwu/framework/tensorflow22/tensorflow/ydwu-learning_tf/quantized_platerec_model_graph_10928f2_opt_reshape.pb --show_flops --input_layer=Placeholder --input_layer_type=float --input_layer_shape= --output_layer=ArgMax


ArgMax/dimension (Const): [], value=Tensor<type: int32 shape: [] values: 2>
Reshape/shape (Const): [], value=Tensor<type: int32 shape: [3] values: -1 8 76>
MobileNet/fc_16/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/fc_16/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2095410816 19 255]>
MobileNet/fc_16/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [255 239 1073741824]...>
MobileNet/fc_16/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [608] values: 203 227 223...>
MobileNet/fc_16/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1605920000 5 255]>
MobileNet/fc_16/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [145 204]>
MobileNet/fc_16/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1024,608] values: [201 199 201]...>
MobileNet/ReshapeModel/shape (Const): [], value=Tensor<type: int32 shape: [2] values: 1 1024>
MobileNet/conv_ds_14/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [145]>
MobileNet/conv_ds_14/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_14/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1454115712 18 145]>
MobileNet/conv_ds_14/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [99 189 1073741824]...>
MobileNet/conv_ds_14/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1024] values: 108 151 118...>
MobileNet/conv_ds_14/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1696724864 8 99]>
MobileNet/conv_ds_14/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [127 121]>
MobileNet/conv_ds_14/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,1024,1024] values: [[[128 116 131]]]...>
MobileNet/conv_ds_14/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [127]>
MobileNet/conv_ds_14/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_14/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2104198656 19 127]>
MobileNet/conv_ds_14/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [128 94 1073741824]...>
MobileNet/conv_ds_14/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1024] values: 68 89 86...>
MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1321174400 4 128]>
MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [138 73]>
MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,1024,1] values: [[[53][73][69]]]...>
MobileNet/conv_ds_13/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [138]>
MobileNet/conv_ds_13/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_13/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1723857408 18 138]>
MobileNet/conv_ds_13/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [119 157 1073741824]...>
MobileNet/conv_ds_13/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1024] values: 113 127 155...>
MobileNet/conv_ds_13/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1865264768 8 119]>
MobileNet/conv_ds_13/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [115 121]>
MobileNet/conv_ds_13/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,1024] values: [[[133 123 117]]]...>
MobileNet/conv_ds_13/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [115]>
MobileNet/conv_ds_13/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_13/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2066937088 19 115]>
MobileNet/conv_ds_13/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [120 84 1073741824]...>
MobileNet/conv_ds_13/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 114 68 49...>
MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2090812416 5 120]>
MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [138 144]>
MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[136][144][140]]]...>
MobileNet/conv_ds_12/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [138]>
MobileNet/conv_ds_12/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_12/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1239313664 18 138]>
MobileNet/conv_ds_12/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [126 172 1073741824]...>
MobileNet/conv_ds_12/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 138 155 176...>
MobileNet/conv_ds_12/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1396147968 7 126]>
MobileNet/conv_ds_12/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [118 148]>
MobileNet/conv_ds_12/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,512] values: [[[140 140 163]]]...>
MobileNet/conv_ds_12/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [118]>
MobileNet/conv_ds_12/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_12/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1100123392 18 118]>
MobileNet/conv_ds_12/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [113 122 1073741824]...>
MobileNet/conv_ds_12/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 80 223 154...>
MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1098821248 5 113]>
MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [131 104]>
MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[103][101][111]]]...>
MobileNet/conv_ds_11/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [131]>
MobileNet/conv_ds_11/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_11/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1241812224 18 131]>
MobileNet/conv_ds_11/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [130 137 1073741824]...>
MobileNet/conv_ds_11/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 126 132 135...>
MobileNet/conv_ds_11/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1263318144 7 130]>
MobileNet/conv_ds_11/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [113 132]>
MobileNet/conv_ds_11/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,512] values: [[[128 135 132]]]...>
MobileNet/conv_ds_11/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [113]>
MobileNet/conv_ds_11/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_11/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1102631680 18 113]>
MobileNet/conv_ds_11/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [107 95 1073741824]...>
MobileNet/conv_ds_11/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 13 70 89...>
MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1834800384 6 107]>
MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [129 106]>
MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[32][77][98]]]...>
MobileNet/conv_ds_10/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [129]>
MobileNet/conv_ds_10/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_10/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1168392192 18 129]>
MobileNet/conv_ds_10/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [131 130 1073741824]...>
MobileNet/conv_ds_10/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 90 103 170...>
MobileNet/conv_ds_10/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1185426304 7 131]>
MobileNet/conv_ds_10/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [119 120]>
MobileNet/conv_ds_10/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,512] values: [[[134 138 109]]]...>
MobileNet/conv_ds_10/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [119]>
MobileNet/conv_ds_10/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_10/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1092593792 18 119]>
MobileNet/conv_ds_10/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [111 125 1073741824]...>
MobileNet/conv_ds_10/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 95 102 209...>
MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1979035392 6 111]>
MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [128 129]>
MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[104][128][112]]]...>
MobileNet/conv_ds_9/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [128]>
MobileNet/conv_ds_9/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_9/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1279086592 18 128]>
MobileNet/conv_ds_9/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [128 135 1073741824]...>
MobileNet/conv_ds_9/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 150 151 129...>
MobileNet/conv_ds_9/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1577905792 8 128]>
MobileNet/conv_ds_9/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [108 124]>
MobileNet/conv_ds_9/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,512] values: [[[106 116 106]]]...>
MobileNet/conv_ds_9/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [108]>
MobileNet/conv_ds_9/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_9/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1073842048 18 108]>
MobileNet/conv_ds_9/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [104 112 1073741824]...>
MobileNet/conv_ds_9/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 78 72 146...>
MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1755710080 3 104]>
MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [126 235]>
MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[236][235][243]]]...>
MobileNet/conv_ds_8/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [126]>
MobileNet/conv_ds_8/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_8/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1224328064 18 126]>
MobileNet/conv_ds_8/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [133 107 1073741824]...>
MobileNet/conv_ds_8/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 79 56 113...>
MobileNet/conv_ds_8/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1694755968 8 133]>
MobileNet/conv_ds_8/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [108 125]>
MobileNet/conv_ds_8/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,512,512] values: [[[133 123 113]]]...>
MobileNet/conv_ds_8/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [108]>
MobileNet/conv_ds_8/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_8/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2123665280 19 108]>
MobileNet/conv_ds_8/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [107 103 1073741824]...>
MobileNet/conv_ds_8/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 111 146 159...>
MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1255182464 5 107]>
MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [119 124]>
MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,512,1] values: [[[119][101][107]]]...>
MobileNet/conv_ds_7/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [119]>
MobileNet/conv_ds_7/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_7/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1366457984 18 119]>
MobileNet/conv_ds_7/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [136 129 1073741824]...>
MobileNet/conv_ds_7/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [512] values: 159 203 184...>
MobileNet/conv_ds_7/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1586226432 8 136]>
MobileNet/conv_ds_7/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [89 139]>
MobileNet/conv_ds_7/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,256,512] values: [[[130 160 126]]]...>
MobileNet/conv_ds_7/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [89]>
MobileNet/conv_ds_7/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_7/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1096427136 18 89]>
MobileNet/conv_ds_7/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [104 103 1073741824]...>
MobileNet/conv_ds_7/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [256] values: 94 138 224...>
MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1878343552 6 104]>
MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [128 129]>
MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,256,1] values: [[[145][163][114]]]...>
MobileNet/conv_ds_6/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [128]>
MobileNet/conv_ds_6/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_6/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1083421696 18 128]>
MobileNet/conv_ds_6/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [130 74 1073741824]...>
MobileNet/conv_ds_6/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [256] values: 63 96 103...>
MobileNet/conv_ds_6/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1838268160 8 130]>
MobileNet/conv_ds_6/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [106 146]>
MobileNet/conv_ds_6/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,256,256] values: [[[175 111 130]]]...>
MobileNet/conv_ds_6/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [106]>
MobileNet/conv_ds_6/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_6/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1089759232 18 106]>
MobileNet/conv_ds_6/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [109 98 1073741824]...>
MobileNet/conv_ds_6/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [256] values: 91 52 62...>
MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2026233216 6 109]>
MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [107 107]>
MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,256,1] values: [[[88][119][100]]]...>
MobileNet/conv_ds_5/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [107]>
MobileNet/conv_ds_5/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_5/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1362231936 18 107]>
MobileNet/conv_ds_5/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [142 76 1073741824]...>
MobileNet/conv_ds_5/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [256] values: 135 119 72...>
MobileNet/conv_ds_5/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1987316608 8 142]>
MobileNet/conv_ds_5/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [90 138]>
MobileNet/conv_ds_5/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,128,256] values: [[[136 90 146]]]...>
MobileNet/conv_ds_5/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [90]>
MobileNet/conv_ds_5/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_5/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1175513344 18 90]>
MobileNet/conv_ds_5/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [107 92 1073741824]...>
MobileNet/conv_ds_5/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [128] values: 91 126 182...>
MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1283245952 6 107]>
MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [126 108]>
MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,128,1] values: [[[111][133][119]]]...>
MobileNet/conv_ds_4/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [126]>
MobileNet/conv_ds_4/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_4/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2104597504 19 126]>
MobileNet/conv_ds_4/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [134 58 1073741824]...>
MobileNet/conv_ds_4/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [128] values: 20 76 73...>
MobileNet/conv_ds_4/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1513330432 7 134]>
MobileNet/conv_ds_4/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [114 149]>
MobileNet/conv_ds_4/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,128,128] values: [[[120 133 141]]]...>
MobileNet/conv_ds_4/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [114]>
MobileNet/conv_ds_4/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_4/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1193847168 18 114]>
MobileNet/conv_ds_4/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [119 146 1073741824]...>
MobileNet/conv_ds_4/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [128] values: 64 148 130...>
MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1768808576 6 119]>
MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [122 100]>
MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,128,1] values: [[[97][115][88]]]...>
MobileNet/conv_ds_3/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [122]>
MobileNet/conv_ds_3/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_3/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1137917184 18 122]>
MobileNet/conv_ds_3/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [128 88 1073741824]...>
MobileNet/conv_ds_3/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [128] values: 78 100 135...>
MobileNet/conv_ds_3/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2114255232 7 128]>
MobileNet/conv_ds_3/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [115 139]>
MobileNet/conv_ds_3/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,64,128] values: [[[151 155 157]]]...>
MobileNet/conv_ds_3/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [115]>
MobileNet/conv_ds_3/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_3/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1208816896 18 115]>
MobileNet/conv_ds_3/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [123 138 1073741824]...>
MobileNet/conv_ds_3/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [64] values: 137 226 0...>
MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1075179392 5 123]>
MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [129 118]>
MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,64,1] values: [[[70][113][121]]]...>
MobileNet/conv_ds_2/pointwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [129]>
MobileNet/conv_ds_2/pointwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_2/pointwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1081026432 18 129]>
MobileNet/conv_ds_2/pointwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [140 68 1073741824]...>
MobileNet/conv_ds_2/pointwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [64] values: 134 119 122...>
MobileNet/conv_ds_2/pointwise_conv/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1313850624 6 140]>
MobileNet/conv_ds_2/pointwise_conv/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [125 134]>
MobileNet/conv_ds_2/pointwise_conv/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [1,1,32,64] values: [[[134 117 101]]]...>
MobileNet/conv_ds_2/depthwise_conv/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [125]>
MobileNet/conv_ds_2/depthwise_conv/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_ds_2/depthwise_conv/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1083751552 18 125]>
MobileNet/conv_ds_2/depthwise_conv/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [124 130 1073741824]...>
MobileNet/conv_ds_2/depthwise_conv/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [32] values: 128 62 183...>
MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1582404608 5 124]>
MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [114 123]>
MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,32,1] values: [[[103][125][140]]]...>
MobileNet/conv_1/BiasAdd_relu_min_quantized (Const): [], value=Tensor<type: int32 shape: [1,1] values: [114]>
MobileNet/conv_1/biases_output_min_max (Const): [], value=Tensor<type: float shape: [1,2] values: [-356.478 -0.672853]>
MobileNet/conv_1/biases_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [2105605120 19 114]>
MobileNet/conv_1/biases_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,6] values: [140 100 1073741824]...>
MobileNet/conv_1/biases_quint8_const (Const): [], value=Tensor<type: quint8 shape: [32] values: 145 182 206...>
MobileNet/conv_1/convolution_eightbit_reduction_dims (Const): [], value=Tensor<type: int32 shape: [1] values: 0>
MobileNet/conv_1/convolution_eightbit_reshape_dims (Const): [], value=Tensor<type: int32 shape: [1] values: -1>
MobileNet/conv_1/weights_deep_quantize_parameter (Const): [], value=Tensor<type: int32 shape: [1,3] values: [1978518784 8 140]>
MobileNet/conv_1/weights_input_filter_offset (Const): [], value=Tensor<type: int32 shape: [1,2] values: [128 128]>
MobileNet/conv_1/weights_quint8_const (Const): [], value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[120 116 110]]]...>
ExpandDims/dim (Const): [], value=Tensor<type: int32 shape: [] values: 0>



Placeholder (Placeholder): []
ExpandDims (ExpandDims): [Placeholder, ExpandDims/dim]
MobileNet/conv_1/convolution_eightbit_reshape_ExpandDims (Reshape): [ExpandDims, MobileNet/conv_1/convolution_eightbit_reshape_dims]
MobileNet/conv_1/convolution_eightbit_max_ExpandDims (Max): [MobileNet/conv_1/convolution_eightbit_reshape_ExpandDims, MobileNet/conv_1/convolution_eightbit_reduction_dims]
MobileNet/conv_1/convolution_eightbit_min_ExpandDims (Min): [MobileNet/conv_1/convolution_eightbit_reshape_ExpandDims, MobileNet/conv_1/convolution_eightbit_reduction_dims]
MobileNet/conv_1/convolution_eightbit_quantize_ExpandDims (QuantizeV2): [ExpandDims, MobileNet/conv_1/convolution_eightbit_min_ExpandDims, MobileNet/conv_1/convolution_eightbit_max_ExpandDims]
MobileNet/conv_1/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_1/convolution_eightbit_quantize_ExpandDims, MobileNet/conv_1/weights_quint8_const, MobileNet/conv_1/weights_input_filter_offset, MobileNet/conv_1/weights_deep_quantize_parameter]
MobileNet/conv_1/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_1/convolution_eightbit_quantized_conv, MobileNet/conv_1/biases_quint8_const, MobileNet/conv_1/biases_input_filter_offset, MobileNet/conv_1/biases_deep_quantize_parameter, MobileNet/conv_1/biases_output_min_max]
MobileNet/conv_1/batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_1/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_1/BiasAdd_relu_min_quantized, MobileNet/conv_1/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_1/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_2/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_1/batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_2/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_2/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_2/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_2/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_2/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_2/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_2/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_2/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_2/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_2/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_2/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_2/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_2/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_2/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_2/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_2/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_2/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_2/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_2/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_2/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_2/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_2/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_2/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_2/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_2/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_2/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_2/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_3/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_2/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_3/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_3/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_3/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_3/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_3/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_3/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_3/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_3/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_3/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_3/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_3/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_3/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_3/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_3/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_3/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_3/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_3/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_3/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_3/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_3/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_3/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_3/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_3/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_3/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_3/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_3/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_3/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_4/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_3/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_4/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_4/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_4/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_4/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_4/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_4/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_4/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_4/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_4/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_4/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_4/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_4/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_4/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_4/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_4/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_4/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_4/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_4/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_4/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_4/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_4/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_4/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_4/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_4/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_4/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_4/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_4/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_4/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_5/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_4/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_5/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_5/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_5/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_5/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_5/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_5/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_5/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_5/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_5/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_5/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_5/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_5/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_5/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_5/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_5/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_5/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_5/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_5/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_5/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_5/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_5/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_5/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_5/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_5/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_5/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_5/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_5/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_5/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_6/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_5/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_6/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_6/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_6/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_6/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_6/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_6/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_6/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_6/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_6/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_6/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_6/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_6/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_6/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_6/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_6/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_6/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_6/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_6/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_6/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_6/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_6/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_6/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_6/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_6/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_6/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_6/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_6/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_7/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_6/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_7/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_7/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_7/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_7/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_7/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_7/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_7/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_7/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_7/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_7/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_7/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_7/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_7/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_7/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_7/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_7/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_7/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_7/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_7/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_7/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_7/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_7/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_7/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_7/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_7/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_7/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_7/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_7/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_8/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_7/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_8/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_8/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_8/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_8/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_8/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_8/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_8/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_8/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_8/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_8/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_8/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_8/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_8/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_8/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_8/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_8/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_8/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_8/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_8/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_8/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_8/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_8/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_8/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_8/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_8/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_8/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_8/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_8/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_9/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_8/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_9/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_9/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_9/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_9/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_9/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_9/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_9/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_9/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_9/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_9/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_9/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_9/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_9/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_9/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_9/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_9/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_9/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_9/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_9/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_9/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_9/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_9/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_9/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_9/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_9/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_9/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_9/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_9/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_10/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_9/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_10/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_10/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_10/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_10/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_10/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_10/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_10/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_10/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_10/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_10/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_10/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_10/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_10/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_10/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_10/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_10/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_10/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_10/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_10/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_10/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_10/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_10/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_10/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_10/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_10/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_10/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_10/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_10/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_11/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_10/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_11/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_11/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_11/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_11/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_11/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_11/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_11/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_11/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_11/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_11/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_11/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_11/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_11/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_11/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_11/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_11/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_11/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_11/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_11/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_11/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_11/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_11/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_11/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_11/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_11/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_11/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_11/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_11/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_12/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_11/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_12/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_12/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_12/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_12/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_12/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_12/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_12/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_12/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_12/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_12/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_12/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_12/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_12/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_12/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_12/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_12/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_12/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_12/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_12/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_12/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_12/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_12/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_12/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_12/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_12/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_12/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_12/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_12/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_13/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_12/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_13/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_13/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_13/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_13/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_13/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_13/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_13/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_13/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_13/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_13/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_13/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_13/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_13/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_13/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_13/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_13/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_13/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_13/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_13/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_13/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_13/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_13/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_13/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_13/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_13/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_13/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_13/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_13/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_14/depthwise_conv/depthwise_eightbit_quantized_depthwise (QuantizedDepthwiseConv): [MobileNet/conv_ds_13/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_quint8_const, MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_input_filter_offset, MobileNet/conv_ds_14/depthwise_conv/depthwise_weights_deep_quantize_parameter]
MobileNet/conv_ds_14/depthwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_14/depthwise_conv/depthwise_eightbit_quantized_depthwise, MobileNet/conv_ds_14/depthwise_conv/biases_quint8_const, MobileNet/conv_ds_14/depthwise_conv/biases_input_filter_offset, MobileNet/conv_ds_14/depthwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_14/depthwise_conv/biases_output_min_max]
MobileNet/conv_ds_14/dw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_14/depthwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_14/depthwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_14/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_14/depthwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/conv_ds_14/pointwise_conv/convolution_eightbit_quantized_conv (QuantizedConv2D): [MobileNet/conv_ds_14/dw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_14/pointwise_conv/weights_quint8_const, MobileNet/conv_ds_14/pointwise_conv/weights_input_filter_offset, MobileNet/conv_ds_14/pointwise_conv/weights_deep_quantize_parameter]
MobileNet/conv_ds_14/pointwise_conv/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/conv_ds_14/pointwise_conv/convolution_eightbit_quantized_conv, MobileNet/conv_ds_14/pointwise_conv/biases_quint8_const, MobileNet/conv_ds_14/pointwise_conv/biases_input_filter_offset, MobileNet/conv_ds_14/pointwise_conv/biases_deep_quantize_parameter, MobileNet/conv_ds_14/pointwise_conv/biases_output_min_max]
MobileNet/conv_ds_14/pw_batch_norm/Relu_eightbit_quantized (QuantizedRelu): [MobileNet/conv_ds_14/pointwise_conv/BiasAdd_eightbit_quantized_bias_add, MobileNet/conv_ds_14/pointwise_conv/BiasAdd_relu_min_quantized, MobileNet/conv_ds_14/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/conv_ds_14/pointwise_conv/BiasAdd_eightbit_quantized_bias_add:2]
MobileNet/avg_pool_15/AvgPool_eightbit_quantized (QuantizedAvgPool): [MobileNet/conv_ds_14/pw_batch_norm/Relu_eightbit_quantized, MobileNet/conv_ds_14/pw_batch_norm/Relu_eightbit_quantized:1, MobileNet/conv_ds_14/pw_batch_norm/Relu_eightbit_quantized:2]
MobileNet/ReshapeModel_eightbit_quantized_reshape (QuantizedReshape): [MobileNet/avg_pool_15/AvgPool_eightbit_quantized, MobileNet/ReshapeModel/shape, MobileNet/avg_pool_15/AvgPool_eightbit_quantized:1, MobileNet/avg_pool_15/AvgPool_eightbit_quantized:2]
MobileNet/fc_16/MatMul_eightbit_quantized_mat_mul (QuantizedMatMul): [MobileNet/ReshapeModel_eightbit_quantized_reshape, MobileNet/fc_16/weights_quint8_const, MobileNet/ReshapeModel_eightbit_quantized_reshape:1, MobileNet/ReshapeModel_eightbit_quantized_reshape:2, MobileNet/fc_16/weights_input_filter_offset, MobileNet/fc_16/weights_deep_quantize_parameter]
MobileNet/fc_16/BiasAdd_eightbit_quantized_bias_add (QuantizedBiasAdd): [MobileNet/fc_16/MatMul_eightbit_quantized_mat_mul, MobileNet/fc_16/biases_quint8_const, MobileNet/fc_16/biases_input_filter_offset, MobileNet/fc_16/biases_deep_quantize_parameter, MobileNet/fc_16/biases_output_min_max]
Reshape_eightbit_quantized_reshape (QuantizedReshape): [MobileNet/fc_16/BiasAdd_eightbit_quantized_bias_add, Reshape/shape, MobileNet/fc_16/BiasAdd_eightbit_quantized_bias_add:1, MobileNet/fc_16/BiasAdd_eightbit_quantized_bias_add:2]
Reshape (Dequantize): [Reshape_eightbit_quantized_reshape, Reshape_eightbit_quantized_reshape:1, Reshape_eightbit_quantized_reshape:2]
ArgMax (ArgMax): [Reshape, ArgMax/dimension]